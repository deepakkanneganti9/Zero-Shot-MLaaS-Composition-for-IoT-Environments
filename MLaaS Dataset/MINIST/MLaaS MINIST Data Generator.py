# -*- coding: utf-8 -*-
"""MINIST_MLaaS_Composition_Dataset_before making_for_py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o5GJZ1PhQWwm4uh67it6z-hS1PsJWnT8

**MINIST Dataset for IID and Non-IID**
---
"""
#!pip install tensorflow
# Cell 1: Load MNIST and convert to NumPy arrays
import tensorflow as tf
import numpy as np
import os
os.environ["CUDA_VISIBLE_DEVICES"]="-1"
# Load the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()
# Combine and convert to NumPy
all_images = np.concatenate([train_images, test_images], axis=0)
all_labels = np.concatenate([train_labels, test_labels], axis=0)
# Convert to float32 for consistency (optional)
all_images = all_images.astype(np.float32)
all_labels = all_labels.astype(np.int32)
# Print the total number of samples
print("Total number of samples in MNIST dataset:", len(all_images))

# Cell 2: Create flexible client distribution (user-defined IID clients)
num_clients = 25
def create_clients(images, labels, num_clients, iid_client_ids):
    clients_data = {}
    total_samples = len(images)
    # Shuffle the dataset
    indices = np.arange(total_samples)
    np.random.shuffle(indices)
    images, labels = images[indices], labels[indices]
    samples_per_client = total_samples // num_clients
    start_idx = 0
    for i in range(num_clients):
        if i in iid_client_ids:
            # IID client: get balanced random data
            client_images = images[start_idx:start_idx + samples_per_client]
            client_labels = labels[start_idx:start_idx + samples_per_client]
            start_idx += samples_per_client
        else:
            # Non-IID client: use biased label distribution
            non_iid_classes = 5
            chosen_classes = np.random.choice(10, non_iid_classes, replace=False)
            mask = np.isin(labels, chosen_classes)
            filtered_images = images[mask]
            filtered_labels = labels[mask]
            # Random sample from filtered data
            noniid_indices = np.random.choice(len(filtered_images), samples_per_client, replace=False)
            client_images = filtered_images[noniid_indices]
            client_labels = filtered_labels[noniid_indices]
        clients_data[f"client_{i+1}"] = (client_images, client_labels)
    return clients_data

# Cell 3: Create and inspect IID and non-IID clients correctly
iid_client_indices = [0, 2, 4, 6, 8,10,12,14,16,18,20,22,24]       # clients 1,3,5,7,9 are IID
non_iid_client_indices = [1, 3, 5, 7, 9,11,13,15,17,19,21,23,25]   # clients 2,4,6,8,10 are non-IID
overlap = set(iid_client_indices) & set(non_iid_client_indices)
if overlap:
    raise ValueError(f"Overlap found in IID and Non-IID clients: {overlap}")
def create_clients(images, labels, num_clients, iid_clients, non_iid_clients):
    clients_data = {}
    total_samples = len(images)
    samples_per_client = total_samples // num_clients

    # Prepare per-class indices
    class_indices = {cls: np.where(labels == cls)[0] for cls in range(10)}

    for i in range(num_clients):
        client_samples = []

        # IID client ‚Üí uniform sample per class
        if i in iid_clients:
            for cls in range(10):
                chosen = np.random.choice(class_indices[cls], samples_per_client // 10, replace=False)
                client_samples.extend(chosen)

        # Non-IID client ‚Üí same classes, but skewed proportions
        elif i in non_iid_clients:
            # Generate skewed weights for each class (some dominant)
            class_weights = np.random.dirichlet(np.ones(10) * np.random.uniform(0.3, 3))
            class_weights /= class_weights.sum()

            for cls in range(10):
                n_cls = int(class_weights[cls] * samples_per_client)
                chosen = np.random.choice(class_indices[cls], n_cls, replace=True)
                client_samples.extend(chosen)
        else:
            continue

        np.random.shuffle(client_samples)
        client_images = images[client_samples]
        client_labels = labels[client_samples]

        clients_data[f"client_{i+1}"] = (client_images, client_labels)

    return clients_data


# üîÅ Create dataset
clients_data = create_clients(all_images, all_labels, num_clients,
                              iid_clients=iid_client_indices,
                              non_iid_clients=non_iid_client_indices)

# üìä Print distributions for a few clients
for client_id in ['client_1', 'client_2','client_3', 'client_4','client_5', 'client_6','client_7','client_8','client_9','client_10',
                  'client_11', 'client_12','client_13', 'client_14','client_15', 'client_16','client_17','client_18','client_19','client_20',
                  'client_21', 'client_22','client_23', 'client_24','client_25']:
    if client_id in clients_data:
        _, client_labels = clients_data[client_id]
        unique, counts = np.unique(client_labels, return_counts=True)
        print(f"Label distribution for {client_id}: {dict(zip(unique, counts))}")

"""**MLaaS Clint signature uisng MINIST dataset**
---
"""

# Cell 0 ‚Äì Setup environment and imports
#pip install tensorflow==2.16.1  # Uncomment in Colab if TF not installed

import numpy as np, pandas as pd, random, time, os

try:
    import tensorflow as tf
    from tensorflow.keras import layers, models
    TF_AVAILABLE = True
except Exception as e:
    print("TensorFlow unavailable; will simulate instead.\n", e)
    TF_AVAILABLE = False

random.seed(42)
np.random.seed(42)
if TF_AVAILABLE:
    tf.random.set_seed(42)

# Cell 1 ‚Äì Use pre-created IID / Non-IID clients
# You already have `clients_data` ready.
# Format: clients_data["client_1"] = (images, labels)

print(f"‚úÖ Loaded {len(clients_data)} clients from your custom IID/Non-IID setup.")
sample_client = random.choice(list(clients_data.keys()))
print(f"Example client: {sample_client}, data shape = {clients_data[sample_client][0].shape}")

# Cell 2 ‚Äì Configuration and Noise Functions

NUM_CLIENTS = len(clients_data)
FEATURE_COUNT = 28 * 28
EPOCHS = 5
BATCH = 64
MAX_SLOTS = 5

PIXEL_NOISE_CLIENTS = {3, 4, 5, 6}
LABEL_NOISE_CLIENTS = {5, 6, 9}
SHIFT_CLIENTS = {2, 5, 6, 7}
GLOBAL_NOISE_SEED = 1234
np.random.seed(GLOBAL_NOISE_SEED)

def apply_pixel_noise(x, noise_std=0.25):
    rng = np.random.default_rng(GLOBAL_NOISE_SEED)
    noise = rng.normal(0, noise_std, x.shape).astype("float32")
    return np.clip(x + noise, 0.0, 1.0)

def apply_label_noise(y, frac=0.25, num_classes=10):
    y = y.copy()
    rng = np.random.default_rng(GLOBAL_NOISE_SEED)
    n = len(y)
    k = int(frac * n)
    if k > 0:
        idx = rng.choice(n, size=k, replace=False)
        y[idx] = rng.integers(0, num_classes, size=k)
    return y

def apply_distribution_shift(x):
    c = 1.4
    b = 0.2
    return np.clip(c * x + b, 0.0, 1.0)

# Cell 3 ‚Äì Define CNN model and global test data
if TF_AVAILABLE:
    def build_model():
        model = models.Sequential([
            layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
            layers.MaxPooling2D((2,2)),
            layers.Flatten(),
            layers.Dense(64, activation='relu'),
            layers.Dense(10, activation='softmax')
        ])
        model.compile(optimizer='adam',
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])
        return model

    # Create global test set from client 1‚Äôs data for simplicity
    all_images = np.concatenate([v[0] for v in clients_data.values()])
    all_labels = np.concatenate([v[1] for v in clients_data.values()])
    perm = np.random.permutation(len(all_images))
    x_test = (all_images[perm[:10000]] / 255.0).astype("float32")[..., None]
    y_test = all_labels[perm[:10000]]

# Cell 4 ‚Äì Train local models per client, record metrics, and export CSV
local_results = {}
label_distributions = {}
records = []  # for direct CSV export

if TF_AVAILABLE:
    for cid, (x_c, y_c) in enumerate(clients_data.values(), start=1):
        # Normalize and reshape
        x_c = (x_c / 255.0).astype("float32")[..., None]
        y_c = y_c.astype("int32")

        # Apply deterministic noise patterns
        if cid in PIXEL_NOISE_CLIENTS:
            x_c = apply_pixel_noise(x_c)
        if cid in LABEL_NOISE_CLIENTS:
            y_c = apply_label_noise(y_c)
        if cid in SHIFT_CLIENTS:
            x_c = apply_distribution_shift(x_c)

        # Label distribution
        label_counts = dict(zip(*np.unique(y_c, return_counts=True)))
        label_distributions[cid] = label_counts

        # Model training
        model = build_model()
        t0 = time.time()
        model.fit(x_c, y_c, epochs=EPOCHS, batch_size=BATCH, verbose=0)
        latency_ms = (time.time() - t0) * 1000.0

        loss, acc = model.evaluate(x_test, y_test, verbose=0)

        # Store in-memory results
        local_results[cid] = {
            "samples": int(x_c.shape[0]),
            "features": FEATURE_COUNT,
            "acc": float(acc * 100),
            "latency_ms": float(latency_ms),
            "weights": model.get_weights()
        }

        # Prepare a row for CSV
        row = {
            "Client_ID": cid,
            "DataVolume(Samples)": int(x_c.shape[0]),
            "FeatureCount": FEATURE_COUNT,
            "Local_Accuracy(%)": round(float(acc * 100), 2),
            "Latency(ms)": round(float(latency_ms), 2)
        }
        # Add label distribution columns
        for lbl in range(10):
            row[f"Label_{lbl}_Count"] = label_counts.get(lbl, 0)

        records.append(row)

    # Convert all local results to CSV
    df_local = pd.DataFrame(records)
    OUT_LOCAL_CSV = "MNIST_Local_Client_Statistics.csv"
    df_local.to_csv(OUT_LOCAL_CSV, index=False)
    print(f"‚úÖ Local training completed for {len(local_results)} clients.")
    print(f"üìÅ Local statistics saved to: {OUT_LOCAL_CSV}")
else:
    print("‚ö†Ô∏è TensorFlow not available; simulation mode not included in this version.")

"""**MLaaS Service Signature**

"""

#import numpy as np, pandas as pd, random, time, os
#df_local=pd.read_csv("/content/MNIST_Local_Client_Statistics.csv")
#df_local

"""**MLaaS Composition Dataset generation**
---
"""

# Cell 5 ‚Äì FedAvg and aggregation functions
def fedavg_weights(clients):
    total = sum(local_results[c]["samples"] for c in clients)
    w_sum = None
    for i, c in enumerate(clients):
        w = local_results[c]["weights"]
        factor = local_results[c]["samples"] / total
        if i == 0:
            w_sum = [wi * factor for wi in w]
        else:
            w_sum = [acc + wi * factor for acc, wi in zip(w_sum, w)]
    return w_sum
def evaluate_global_accuracy(clients):
    gmodel = build_model()
    gmodel.set_weights(fedavg_weights(clients))
    loss, acc = gmodel.evaluate(x_test, y_test, verbose=0)
    return float(acc * 100)

def aggregate_latency(clients):
    lats = [local_results[c]["latency_ms"] for c in clients]
    return float(sum(lats)), float(max(lats))

def aggregate_volume(clients):
    return int(sum(local_results[c]["samples"] for c in clients))

# Cell 6 ‚Äì Evaluate 2- and 3-client combinations for clients 1 to 10 only

from itertools import combinations

target_client_ids = list(range(1, 11))  # C1 to C10 only
combos = list(combinations(target_client_ids, 2))+list(combinations(target_client_ids, 3))+list(combinations(target_client_ids, 4))+list(combinations(target_client_ids, 5))

formatted = ['-'.join([f"C{i}" for i in combo]) for combo in combos]
df_out = pd.DataFrame({"Combination": formatted})

SLOT_SUFFIXES = ["DataVolume(MB)", "FeatureCount", "Accuracy(%)", "Latency(ms)"]
LABEL_COLS = [f"Label{i}" for i in range(10)]

# Initialize client-level columns
for k in range(1, MAX_SLOTS + 1):
    for suf in SLOT_SUFFIXES + LABEL_COLS:
        df_out[f"C{k}_{suf}"] = 0

# Initialize global-level columns
for col in ["Global_Accuracy(%)", "Global_DataVolume", "Global_Latency_Sum(ms)", "Global_RoundTime_Max(ms)"]:
    df_out[col] = 0.0

# Populate values
for i, combo in enumerate(combos):
    for slot, cid in enumerate(combo[:MAX_SLOTS], start=1):
        df_out.at[i, f"C{slot}_DataVolume(MB)"] = local_results[cid]["samples"]
        df_out.at[i, f"C{slot}_FeatureCount"] = FEATURE_COUNT
        df_out.at[i, f"C{slot}_Accuracy(%)"] = local_results[cid]["acc"]
        df_out.at[i, f"C{slot}_Latency(ms)"] = local_results[cid]["latency_ms"]

        for label in range(10):
            df_out.at[i, f"C{slot}_Label{label}"] = label_distributions[cid].get(label, 0)

    # Global metrics
    gacc = evaluate_global_accuracy(combo)
    gsum, gmax = aggregate_latency(combo)
    gvol = aggregate_volume(combo)

    df_out.at[i, "Global_Accuracy(%)"] = gacc
    df_out.at[i, "Global_DataVolume"] = gvol
    df_out.at[i, "Global_Latency_Sum(ms)"] = gsum
    df_out.at[i, "Global_RoundTime_Max(ms)"] = gmax

print(f"‚úÖ Evaluation completed for {len(df_out)} combinations (C2 + C3 of clients 1‚Äì10).")

df_out

"""**Load MLaaS Composition Dataset**
---
"""

# Cell 7 ‚Äì Save and inspect output
OUT_CSV = "MNIST_Federated_Combinations_Realistic.csv"
df_out.to_csv(OUT_CSV,index=False)
print(f"‚úÖ Results saved to: {OUT_CSV}")

"""**Test Dataset for the Zero-short Composition Phase I and Phase II**
---
"""

df_out=pd.read_csv("MNIST_Federated_Combinations_Realistic.csv")
df_out

#df_local

df_local.columns

df_out.columns

import pandas as pd
import numpy as np
from itertools import combinations

# ---------- Load inputs ----------
comb_path = "MNIST_Federated_Combinations_Realistic.csv"
clients_path = "MNIST_Local_Client_Statistics.csv"

df_base = pd.read_csv(comb_path)
df_clients = pd.read_csv(clients_path)

# Keep only needed client range 15..25 for filling
POOL_MIN, POOL_MAX = 15, 25
pool_ids = list(range(POOL_MIN, POOL_MAX + 1))

# Prepare client stats lookup
df_clients = df_clients.set_index("Client_ID")

# ---------- Helpers ----------
def parse_combo(s: str):
    """'C1-C2' -> [1,2]"""
    if pd.isna(s) or not isinstance(s, str) or not s:
        return []
    return [int(tok[1:]) for tok in s.split("-")]

def fmt_combo(ids):
    return "-".join([f"C{int(i)}" for i in ids])

def client_block_prefixes(df):
    """Find which Ck blocks exist in the base schema (e.g., C1..C5)"""
    out = []
    k = 1
    while f"C{k}_DataVolume(MB)" in df.columns:
        out.append(f"C{k}")
        k += 1
    return out

C_BLOCKS = client_block_prefixes(df_base)  # e.g., ['C1','C2','C3','C4','C5']

def fill_block(row_dict, block_prefix, client_id):
    """Fill a single Ck_* block from df_clients[client_id]."""
    stats = df_clients.loc[client_id]
    # NOTE: df has "Ck_DataVolume(MB)" but clients csv column is "DataVolume(Samples)"
    row_dict[f"{block_prefix}_DataVolume(MB)"] = stats["DataVolume(Samples)"]
    row_dict[f"{block_prefix}_FeatureCount"]   = stats["FeatureCount"]
    row_dict[f"{block_prefix}_Accuracy(%)"]    = stats["Local_Accuracy(%)"]
    row_dict[f"{block_prefix}_Latency(ms)"]    = stats["Latency(ms)"]
    for j in range(10):
        row_dict[f"{block_prefix}_Label{j}"] = stats[f"Label_{j}_Count"]

def zero_block(row_dict, block_prefix):
    """Zero a Ck_* block (when not used)."""
    row_dict[f"{block_prefix}_DataVolume(MB)"] = 0
    row_dict[f"{block_prefix}_FeatureCount"]   = 0
    row_dict[f"{block_prefix}_Accuracy(%)"]    = 0
    row_dict[f"{block_prefix}_Latency(ms)"]    = 0
    for j in range(10):
        row_dict[f"{block_prefix}_Label{j}"] = 0

def compute_globals(row_dict):
    """Compute global metrics from filled C blocks."""
    accs, dvs, lats = [], [], []
    for blk in C_BLOCKS:
        a  = row_dict.get(f"{blk}_Accuracy(%)", np.nan)
        dv = row_dict.get(f"{blk}_DataVolume(MB)", np.nan)
        lt = row_dict.get(f"{blk}_Latency(ms)", np.nan)
        if pd.notna(a)  and a  != 0: accs.append(float(a))
        if pd.notna(dv) and dv != 0: dvs.append(float(dv))
        if pd.notna(lt) and lt != 0: lats.append(float(lt))
    row_dict["Global_Accuracy(%)"]     = float(np.mean(accs)) if accs else np.nan
    row_dict["Global_DataVolume"]      = float(np.sum(dvs)) if dvs else 0.0
    row_dict["Global_Latency_Sum(ms)"] = float(np.sum(lats)) if lats else 0.0
    row_dict["Global_RoundTime_Max(ms)"]= float(np.max(lats)) if lats else 0.0

# ---------- Filter to only len 2,3,4,5 (we keep 5 as-is) ----------
df_base["Combo_List"] = df_base["Combination"].apply(parse_combo)
df_filtered = df_base[df_base["Combo_List"].apply(lambda x: len(x) in [2,3,4,5])].reset_index(drop=True)

# ---------- Build mapping & filled rows ----------
rows_two_cols = []
rows_filled   = []

for _, base_row in df_filtered.iterrows():
    old_combo = base_row["Combination"]
    old_ids   = parse_combo(old_combo)  # e.g., [1,5] for "C1-C5"
    k_needed  = max(0, len(C_BLOCKS) - len(old_ids))  # fill up to number of blocks (e.g., 5)

    # CASE A: already full length (len == 5) -> keep as-is
    if k_needed == 0:
        rows_two_cols.append({"Old_Combination": fmt_combo(old_ids),
                              "New_Combination": fmt_combo(old_ids)})
        # Copy the row as-is, but ensure globals are consistent
        filled_row = base_row.drop(labels=["Combo_List"]).to_dict()
        filled_row["Old_Combination"] = fmt_combo(old_ids)
        filled_row["New_Combination"] = fmt_combo(old_ids)
        # If you want to recompute globals from existing C-blocks:
        compute_globals(filled_row)
        rows_filled.append(filled_row)
        continue

    # CASE B: need to add clients (len == 2,3,4)
    candidate_pool = [cid for cid in pool_ids if cid not in old_ids]

    # For each completion (choose k_needed clients from pool)
    for add_ids in combinations(candidate_pool, k_needed):
        new_ids = old_ids + list(add_ids)

        # Mapping row
        rows_two_cols.append({"Old_Combination": fmt_combo(old_ids),
                              "New_Combination": fmt_combo(new_ids)})

        # Build a fully filled row using schema of df_base
        row_dict = {col: 0 for col in df_base.columns}  # start with zero-ish
        # Keep identifiers
        row_dict["Old_Combination"] = fmt_combo(old_ids)
        row_dict["New_Combination"] = fmt_combo(new_ids)
        # Preserve "Combination" as the original old combo (if needed elsewhere)
        row_dict["Combination"] = fmt_combo(old_ids)

        # Fill the five C-blocks from the new_ids
        # (i.e., the final composition we evaluate is the new 5-client set)
        ok = True
        for blk, cid in zip(C_BLOCKS, new_ids):
            if cid not in df_clients.index:
                ok = False
                break
            fill_block(row_dict, blk, cid)
        if not ok:
            continue

        # Zero any remaining blocks (shouldn't happen if len(C_BLOCKS)==5 and we fill to 5)
        for blk in C_BLOCKS[len(new_ids):]:
            zero_block(row_dict, blk)

        # Compute globals from the blocks we just filled
        compute_globals(row_dict)
        rows_filled.append(row_dict)

# ---------- DataFrames & Save ----------
df_two_cols = pd.DataFrame(rows_two_cols).drop_duplicates().reset_index(drop=True)
df_filled   = pd.DataFrame(rows_filled)

two_cols_path = "Combinations_Old_vs_New_15to25_All_Len2to5.csv"
filled_path   = "MNIST_Federated_Combinations_Filled_All_Len2to5.csv"

df_two_cols.to_csv(two_cols_path, index=False)
df_filled.to_csv(filled_path, index=False)

print("‚úÖ Files saved:")
print(two_cols_path)
print(filled_path)
print("\nMapping preview:")
print(df_two_cols.head(8))
print("\nFilled preview:")
print(df_filled.head(5))

df_filled.to_csv("few_text_dataset_MINIST.csv")

import pandas as pd

# Load the uploaded CSV file
file_path = "few_text_dataset_MINIST.csv"
df = pd.read_csv(file_path)

# Function to parse combination string like "C1-C2-C3"
def parse_combo(s):
    if pd.isna(s) or not isinstance(s, str) or not s:
        return []
    return [tok for tok in s.split('-') if tok.strip()]

# Filter only rows where Old_Combination length is 3 or 4
df["Old_Length"] = df["Old_Combination"].apply(lambda x: len(parse_combo(x)))
df_filtered = df[df["Old_Length"].isin([3, 4])].reset_index(drop=True)

# Save filtered dataset
filtered_path = "few_text_dataset_MINIST_filtered_3_4.csv"
df_filtered.to_csv(filtered_path, index=False)
df_filtered

df_filtered['Old_Length'].value_counts()

"""**Unique Combinations (Phase I and Phase 2)**
---
"""

import re, random
import pandas as pd

# Load your file (adjust path if needed)
df = pd.read_csv("few_text_dataset_MINIST_filtered_3_4.csv")  # or "/mnt/data/few_text_dataset (1).csv"

def parse_set(s):
    # robust: grabs all integers regardless of spacing or separators
    return frozenset(map(int, re.findall(r"-?\d+", str(s))))

# Parse and make an order-invariant key
df["_set"] = df["New_Combination"].apply(parse_set)
df["_key"] = df["_set"].apply(lambda s: tuple(sorted(s)))

# Drop exact duplicates w.r.t. element set (ignores order)
df_unique = df.drop_duplicates(subset=["_key"]).copy()

# Randomly pick up to 100 unique rows
random_seed = 42  # set for reproducibility (or remove for true randomness)
sample_n = min(500, len(df_unique))
sampled = df_unique.sample(n=sample_n, random_state=random_seed)

# If you want them back as neat strings like "1,2,3"
sampled["normalized_combinations"] = sampled["_key"].apply(lambda t: ",".join(map(str, t)))

# Result: `sampled` holds your unique random 500
print(len(df), "rows ->", len(df_unique), "unique combos -> sampled", len(sampled))
sampled

sampled['Old_Length'].value_counts()

sampled.to_csv("few_text_dataset_1MINIST.csv")

"""**Unique Combinations (Manul Composition)**
---
"""

# =============================
# CONFIGURATION: Load from 'New_Combination'
# =============================
COMBO_CSV_PATH = "few_text_dataset_1MINIST.csv"
df_combos = pd.read_csv(COMBO_CSV_PATH)

combination_list = df_combos["New_Combination"].dropna().unique().tolist()

parsed_combinations = [
    [int(c.replace("C", "")) for c in combo.split("-") if c.startswith("C")]
    for combo in combination_list
]
EPOCHS = 5

# =============================
# Build Model Architecture
# =============================
if TF_AVAILABLE:
    def build_model():
        model = models.Sequential([
            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
            layers.MaxPooling2D((2, 2)),
            layers.Flatten(),
            layers.Dense(64, activation='relu'),
            layers.Dense(10, activation='softmax')
        ])
        model.compile(optimizer='adam',
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])
        return model

# =============================
# Prepare Global Test Set
# =============================
if TF_AVAILABLE:
    all_images = np.concatenate([v[0] for v in clients_data.values()])
    all_labels = np.concatenate([v[1] for v in clients_data.values()])
    perm = np.random.permutation(len(all_images))
    x_test = (all_images[perm[:10000]] / 255.0).astype("float32")[..., None]
    y_test = all_labels[perm[:10000]]

# =============================
# FedAvg + Aggregation Metrics
# =============================
def fedavg_weights(clients):
    total = sum(local_results[c]["samples"] for c in clients)
    w_sum = None
    for i, c in enumerate(clients):
        w = local_results[c]["weights"]
        factor = local_results[c]["samples"] / total
        if i == 0:
            w_sum = [wi * factor for wi in w]
        else:
            w_sum = [acc + wi * factor for acc, wi in zip(w_sum, w)]
    return w_sum

def evaluate_global_accuracy(clients):
    gmodel = build_model()
    gmodel.set_weights(fedavg_weights(clients))
    loss, acc = gmodel.evaluate(x_test, y_test, verbose=0)
    return float(acc * 100)

def aggregate_latency(clients):
    lats = [local_results[c]["latency_ms"] for c in clients]
    return float(sum(lats)), float(max(lats))

def aggregate_volume(clients):
    return int(sum(local_results[c]["samples"] for c in clients))

# =============================
# Simulate for New_Combination Entries
# =============================
combo_records = []

if TF_AVAILABLE:
    for idx, client_ids in enumerate(parsed_combinations[:]):
        start_time = time.time()  # ‚úÖ Start time for this combination

        local_results = {}
        label_distributions = {}
        print(f"\nüîÅ Simulating combination {idx + 1}/{len(parsed_combinations)}: {client_ids}")

        for cid in client_ids:
            x_c, y_c = clients_data[f"client_{cid}"]
            x_c = (x_c / 255.0).astype("float32")[..., None]
            y_c = y_c.astype("int32")

            if cid in PIXEL_NOISE_CLIENTS:
                x_c = apply_pixel_noise(x_c)
            if cid in LABEL_NOISE_CLIENTS:
                y_c = apply_label_noise(y_c)
            if cid in SHIFT_CLIENTS:
                x_c = apply_distribution_shift(x_c)

            model = build_model()
            t0 = time.time()
            model.fit(x_c, y_c, epochs=EPOCHS, batch_size=BATCH, verbose=0)
            latency_ms = (time.time() - t0) * 1000.0
            loss, acc = model.evaluate(x_test, y_test, verbose=0)

            local_results[cid] = {
                "samples": int(x_c.shape[0]),
                "features": FEATURE_COUNT,
                "acc": float(acc * 100),
                "latency_ms": float(latency_ms),
                "weights": model.get_weights()
            }

        # Global aggregation
        gacc = evaluate_global_accuracy(client_ids)
        gsum, gmax = aggregate_latency(client_ids)
        gvol = aggregate_volume(client_ids)

        combo_time = time.time() - start_time  # ‚úÖ Total computation time for this combination

        combo_str = '-'.join([f"C{c}" for c in client_ids])
        combo_records.append({
            "New_Combination": combo_str,
            "Global_Accuracy(%)": gacc,
            "Global_DataVolume": gvol,
            "Global_Latency_Sum(ms)": gsum,
            "Global_RoundTime_Max(ms)": gmax,
            "Combination_Computation_Time(s)": round(combo_time, 3)  # ‚úÖ Added column
        })

# =============================
# Save Final Results from New_Combination
# =============================
df_combo_results = pd.DataFrame(combo_records)
df_combo_results.to_csv("Simulated_Results_From_New_Combinations_ms.csv", index=False)
print("\n‚úÖ Saved simulated results for all New_Combination entries.")

"""Delete all for simulations"""
