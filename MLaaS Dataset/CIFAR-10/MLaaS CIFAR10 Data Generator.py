# -*- coding: utf-8 -*-
"""CIFAR-10_MLaaS Composition Dataset_for_py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iT8E9eVQW8Ul7qKlB1jeFRW_mGR0qrVk
"""

# Cell 1: Load CIFAR-10 and convert to NumPy arrays
import tensorflow as tf
import numpy as np

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# Load the CIFAR-10 dataset (32√ó32 RGB images, 10 classes)
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()

# Flatten label arrays
train_labels = train_labels.flatten()
test_labels = test_labels.flatten()

# Combine and convert to NumPy
all_images = np.concatenate([train_images, test_images], axis=0).astype(np.float32)
all_labels = np.concatenate([train_labels, test_labels], axis=0).astype(np.int32)

# Print dataset info
print("‚úÖ CIFAR-10 dataset loaded successfully.")
print("Total number of samples:", len(all_images))
print("Image shape:", all_images.shape[1:], "| Label range:", np.unique(all_labels))

# Cell 2: Define client configuration
num_clients = 25  # total number of clients
# Define IID and Non-IID clients (0-based indices)
iid_client_indices = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24]
non_iid_client_indices = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23]
# ‚úÖ Check for overlap
overlap = set(iid_client_indices) & set(non_iid_client_indices)
if overlap:
    raise ValueError(f"Overlap found between IID and Non-IID clients: {overlap}")
else:
    print(f"‚úÖ Client configuration set: {len(iid_client_indices)} IID + {len(non_iid_client_indices)} Non-IID clients.")

# Cell 3: Create and inspect IID and Non-IID clients (CIFAR-10)
import numpy as np

def create_clients(images, labels, num_clients, iid_clients, non_iid_clients):
    """
    Creates a mix of IID and Non-IID clients for CIFAR-10.
    IID clients ‚Üí balanced samples across all 10 classes.
    Non-IID clients ‚Üí skewed samples using Dirichlet distribution weights.
    """
    clients_data = {}
    total_samples = len(images)
    samples_per_client = total_samples // num_clients

    # Collect indices for each class
    class_indices = {cls: np.where(labels == cls)[0] for cls in range(10)}

    for i in range(num_clients):
        client_samples = []

        if i in iid_clients:
            # IID: equal number of samples from each class
            per_class = samples_per_client // 10
            for cls in range(10):
                chosen = np.random.choice(class_indices[cls], per_class, replace=False)
                client_samples.extend(chosen)

        elif i in non_iid_clients:
            # Non-IID: skewed label distribution via Dirichlet
            class_weights = np.random.dirichlet(np.ones(10) * np.random.uniform(0.3, 3))
            class_weights /= class_weights.sum()
            for cls in range(10):
                n_cls = max(1, int(class_weights[cls] * samples_per_client))
                chosen = np.random.choice(class_indices[cls], n_cls, replace=True)
                client_samples.extend(chosen)
        else:
            continue

        np.random.shuffle(client_samples)
        client_images = images[client_samples]
        client_labels = labels[client_samples]
        clients_data[f"client_{i+1}"] = (client_images, client_labels)

    return clients_data

# üîÅ Create dataset
clients_data = create_clients(all_images, all_labels, num_clients,
                              iid_clients=iid_client_indices,
                              non_iid_clients=non_iid_client_indices)

# üìä Inspect label distributions for each client
for client_id in [f"client_{i+1}" for i in range(num_clients)]:
    if client_id in clients_data:
        _, client_labels = clients_data[client_id]
        unique, counts = np.unique(client_labels, return_counts=True)
        print(f"{client_id} ‚Üí {dict(zip(unique, counts))}")

"""**MLaaS Clint signature using CIFAR-10 dataset**
---
"""

# Cell 0 ‚Äì Setup environment and imports
# !pip install tensorflow==2.16.1  # Uncomment in Colab if not installed

import numpy as np, pandas as pd, random, time, os

try:
    import tensorflow as tf
    from tensorflow.keras import layers, models
    TF_AVAILABLE = True
except Exception as e:
    print("TensorFlow unavailable; will simulate instead.\n", e)
    TF_AVAILABLE = False

random.seed(42)
np.random.seed(42)
if TF_AVAILABLE:
    tf.random.set_seed(42)

# Cell 3 ‚Äì Configuration and Noise Functions

NUM_CLIENTS = len(clients_data)
FEATURE_COUNT = 32 * 32 * 3
EPOCHS = 5
BATCH = 64
MAX_SLOTS = 5

PIXEL_NOISE_CLIENTS = {3,4,5,6}
LABEL_NOISE_CLIENTS = {5,6,9}
SHIFT_CLIENTS = {2,5,6,7}
GLOBAL_NOISE_SEED = 1234
np.random.seed(GLOBAL_NOISE_SEED)

def apply_pixel_noise(x, noise_std=0.1):
    rng = np.random.default_rng(GLOBAL_NOISE_SEED)
    noise = rng.normal(0, noise_std, x.shape).astype("float32")
    return np.clip(x + noise, 0.0, 255.0)

def apply_label_noise(y, frac=0.25, num_classes=10):
    y = y.copy()
    rng = np.random.default_rng(GLOBAL_NOISE_SEED)
    n = len(y)
    k = int(frac * n)
    if k > 0:
        idx = rng.choice(n, size=k, replace=False)
        y[idx] = rng.integers(0, num_classes, size=k)
    return y

def apply_distribution_shift(x):
    c = 1.1
    b = 10.0
    return np.clip(c * x + b, 0.0, 255.0)

# Cell 4 ‚Äì Define CNN model and global test data (CIFAR-10)
if TF_AVAILABLE:
    def build_model():
        model = models.Sequential([
            layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),
            layers.MaxPooling2D((2,2)),
            layers.Conv2D(64, (3,3), activation='relu'),
            layers.MaxPooling2D((2,2)),
            layers.Flatten(),
            layers.Dense(128, activation='relu'),
            layers.Dense(10, activation='softmax')
        ])
        model.compile(optimizer='adam',
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])
        return model

    # Create global test set
    all_images = np.concatenate([v[0] for v in clients_data.values()])
    all_labels = np.concatenate([v[1] for v in clients_data.values()])
    perm = np.random.permutation(len(all_images))
    x_test = (all_images[perm[:10000]] / 255.0).astype("float32")
    y_test = all_labels[perm[:10000]]

# Cell 5 ‚Äì Train local models per client, record metrics, and export CSV
local_results, label_distributions, records = {}, {}, []

if TF_AVAILABLE:
    for cid, (x_c, y_c) in enumerate(clients_data.values(), start=1):
        x_c = (x_c / 255.0).astype("float32")
        y_c = y_c.astype("int32")

        if cid in PIXEL_NOISE_CLIENTS:
            x_c = apply_pixel_noise(x_c)
        if cid in LABEL_NOISE_CLIENTS:
            y_c = apply_label_noise(y_c)
        if cid in SHIFT_CLIENTS:
            x_c = apply_distribution_shift(x_c)

        label_counts = dict(zip(*np.unique(y_c, return_counts=True)))
        label_distributions[cid] = label_counts

        model = build_model()
        t0 = time.time()
        model.fit(x_c, y_c, epochs=EPOCHS, batch_size=BATCH, verbose=0)
        latency_ms = (time.time() - t0) * 1000.0
        loss, acc = model.evaluate(x_test, y_test, verbose=0)

        local_results[cid] = {
            "samples": int(x_c.shape[0]),
            "features": FEATURE_COUNT,
            "acc": float(acc * 100),
            "latency_ms": float(latency_ms),
            "weights": model.get_weights()
        }

        row = {
            "Client_ID": cid,
            "DataVolume(Samples)": int(x_c.shape[0]),
            "FeatureCount": FEATURE_COUNT,
            "Local_Accuracy(%)": round(float(acc * 100), 2),
            "Latency(ms)": round(float(latency_ms), 2)
        }
        for lbl in range(10):
            row[f"Label_{lbl}_Count"] = label_counts.get(lbl, 0)
        records.append(row)

    df_local = pd.DataFrame(records)
    OUT_LOCAL_CSV = "CIFAR10_Local_Client_Statistics.csv"
    df_local.to_csv(OUT_LOCAL_CSV, index=False)
    print(f"‚úÖ Local training done for {len(local_results)} clients.")
    print(f"üìÅ Saved to: {OUT_LOCAL_CSV}")
else:
    print("‚ö†Ô∏è TensorFlow not available; simulation skipped.")

df_local=pd.read_csv("CIFAR10_Local_Client_Statistics.csv")
df_local

"""**MLaaS Composition Dataset generation (Dont run use the CSV file)**
---
"""

# Cell 6 ‚Äì FedAvg and aggregation functions
def fedavg_weights(clients):
    total = sum(local_results[c]["samples"] for c in clients)
    w_sum = None
    for i, c in enumerate(clients):
        w = local_results[c]["weights"]
        factor = local_results[c]["samples"] / total
        if i == 0:
            w_sum = [wi * factor for wi in w]
        else:
            w_sum = [acc + wi * factor for acc, wi in zip(w_sum, w)]
    return w_sum

def evaluate_global_accuracy(clients):
    gmodel = build_model()
    gmodel.set_weights(fedavg_weights(clients))
    loss, acc = gmodel.evaluate(x_test, y_test, verbose=0)
    return float(acc * 100)

def aggregate_latency(clients):
    lats = [local_results[c]["latency_ms"] for c in clients]
    return float(sum(lats)), float(max(lats))

def aggregate_volume(clients):
    return int(sum(local_results[c]["samples"] for c in clients))

# Cell 7 ‚Äì Evaluate 2-, 3-, and 5-client combinations (CIFAR-10)
from itertools import combinations

target_client_ids = list(range(1, 11))
combos = list(combinations(target_client_ids, 2)) + \
         list(combinations(target_client_ids, 3)) + \
         list(combinations(target_client_ids,4)) + \
         list(combinations(target_client_ids, 5))

formatted = ['-'.join([f"C{i}" for i in combo]) for combo in combos]
df_out = pd.DataFrame({"Combination": formatted})

SLOT_SUFFIXES = ["DataVolume(MB)", "FeatureCount", "Accuracy(%)", "Latency(ms)"]
LABEL_COLS = [f"Label{i}" for i in range(10)]

for k in range(1, MAX_SLOTS + 1):
    for suf in SLOT_SUFFIXES + LABEL_COLS:
        df_out[f"C{k}_{suf}"] = 0

for col in ["Global_Accuracy(%)", "Global_DataVolume", "Global_Latency_Sum(ms)", "Global_RoundTime_Max(ms)"]:
    df_out[col] = 0.0

for i, combo in enumerate(combos):
    for slot, cid in enumerate(combo[:MAX_SLOTS], start=1):
        df_out.at[i, f"C{slot}_DataVolume(MB)"] = local_results[cid]["samples"]
        df_out.at[i, f"C{slot}_FeatureCount"] = FEATURE_COUNT
        df_out.at[i, f"C{slot}_Accuracy(%)"] = local_results[cid]["acc"]
        df_out.at[i, f"C{slot}_Latency(ms)"] = local_results[cid]["latency_ms"]

        for label in range(10):
            df_out.at[i, f"C{slot}_Label{label}"] = label_distributions[cid].get(label, 0)

    gacc = evaluate_global_accuracy(combo)
    gsum, gmax = aggregate_latency(combo)
    gvol = aggregate_volume(combo)

    df_out.at[i, "Global_Accuracy(%)"] = gacc
    df_out.at[i, "Global_DataVolume"] = gvol
    df_out.at[i, "Global_Latency_Sum(ms)"] = gsum
    df_out.at[i, "Global_RoundTime_Max(ms)"] = gmax

print(f"‚úÖ Evaluation completed for {len(df_out)} combinations (2, 3, 5 clients of 1‚Äì10).")

df_out

"""**Load MLaaS Composition Dataset**
---
"""

# Cell 8 ‚Äì Save final CIFAR-10 federated combination results
OUT_GLOBAL_CSV = "CIFAR10_Federated_Combinations_Realistic.csv"
df_out.to_csv(OUT_GLOBAL_CSV, index=False)
print(f"üìÅ Global combination results saved to: {OUT_GLOBAL_CSV}")

import pandas as pd
import numpy as np
from itertools import combinations

# ---------- Load inputs ----------
comb_path = "CIFAR10_Federated_Combinations_Realistic.csv"
clients_path = "CIFAR10_Local_Client_Statistics.csv"

df_base = pd.read_csv(comb_path)
df_clients = pd.read_csv(clients_path)

# Keep only needed client range 15..25 for filling
POOL_MIN, POOL_MAX = 15, 25
pool_ids = list(range(POOL_MIN, POOL_MAX + 1))

# Prepare client stats lookup
df_clients = df_clients.set_index("Client_ID")

# ---------- Helpers ----------
def parse_combo(s: str):
    """'C1-C2' -> [1,2]"""
    if pd.isna(s) or not isinstance(s, str) or not s:
        return []
    return [int(tok[1:]) for tok in s.split("-")]

def fmt_combo(ids):
    return "-".join([f"C{int(i)}" for i in ids])

def client_block_prefixes(df):
    """Find which Ck blocks exist in the base schema (e.g., C1..C5)"""
    out = []
    k = 1
    while f"C{k}_DataVolume(MB)" in df.columns:
        out.append(f"C{k}")
        k += 1
    return out

C_BLOCKS = client_block_prefixes(df_base)  # e.g., ['C1','C2','C3','C4','C5']

def fill_block(row_dict, block_prefix, client_id):
    """Fill a single Ck_* block from df_clients[client_id]."""
    stats = df_clients.loc[client_id]
    # NOTE: df has "Ck_DataVolume(MB)" but clients csv column is "DataVolume(Samples)"
    row_dict[f"{block_prefix}_DataVolume(MB)"] = stats["DataVolume(Samples)"]
    row_dict[f"{block_prefix}_FeatureCount"]   = stats["FeatureCount"]
    row_dict[f"{block_prefix}_Accuracy(%)"]    = stats["Local_Accuracy(%)"]
    row_dict[f"{block_prefix}_Latency(ms)"]    = stats["Latency(ms)"]
    for j in range(10):
        row_dict[f"{block_prefix}_Label{j}"] = stats[f"Label_{j}_Count"]

def zero_block(row_dict, block_prefix):
    """Zero a Ck_* block (when not used)."""
    row_dict[f"{block_prefix}_DataVolume(MB)"] = 0
    row_dict[f"{block_prefix}_FeatureCount"]   = 0
    row_dict[f"{block_prefix}_Accuracy(%)"]    = 0
    row_dict[f"{block_prefix}_Latency(ms)"]    = 0
    for j in range(10):
        row_dict[f"{block_prefix}_Label{j}"] = 0

def compute_globals(row_dict):
    """Compute global metrics from filled C blocks."""
    accs, dvs, lats = [], [], []
    for blk in C_BLOCKS:
        a  = row_dict.get(f"{blk}_Accuracy(%)", np.nan)
        dv = row_dict.get(f"{blk}_DataVolume(MB)", np.nan)
        lt = row_dict.get(f"{blk}_Latency(ms)", np.nan)
        if pd.notna(a)  and a  != 0: accs.append(float(a))
        if pd.notna(dv) and dv != 0: dvs.append(float(dv))
        if pd.notna(lt) and lt != 0: lats.append(float(lt))
    row_dict["Global_Accuracy(%)"]     = float(np.mean(accs)) if accs else np.nan
    row_dict["Global_DataVolume"]      = float(np.sum(dvs)) if dvs else 0.0
    row_dict["Global_Latency_Sum(ms)"] = float(np.sum(lats)) if lats else 0.0
    row_dict["Global_RoundTime_Max(ms)"]= float(np.max(lats)) if lats else 0.0

# ---------- Filter to only len 2,3,4,5 (we keep 5 as-is) ----------
df_base["Combo_List"] = df_base["Combination"].apply(parse_combo)
df_filtered = df_base[df_base["Combo_List"].apply(lambda x: len(x) in [2,3,4,5])].reset_index(drop=True)

# ---------- Build mapping & filled rows ----------
rows_two_cols = []
rows_filled   = []

for _, base_row in df_filtered.iterrows():
    old_combo = base_row["Combination"]
    old_ids   = parse_combo(old_combo)  # e.g., [1,5] for "C1-C5"
    k_needed  = max(0, len(C_BLOCKS) - len(old_ids))  # fill up to number of blocks (e.g., 5)

    # CASE A: already full length (len == 5) -> keep as-is
    if k_needed == 0:
        rows_two_cols.append({"Old_Combination": fmt_combo(old_ids),
                              "New_Combination": fmt_combo(old_ids)})
        # Copy the row as-is, but ensure globals are consistent
        filled_row = base_row.drop(labels=["Combo_List"]).to_dict()
        filled_row["Old_Combination"] = fmt_combo(old_ids)
        filled_row["New_Combination"] = fmt_combo(old_ids)
        # If you want to recompute globals from existing C-blocks:
        compute_globals(filled_row)
        rows_filled.append(filled_row)
        continue

    # CASE B: need to add clients (len == 2,3,4)
    candidate_pool = [cid for cid in pool_ids if cid not in old_ids]

    # For each completion (choose k_needed clients from pool)
    for add_ids in combinations(candidate_pool, k_needed):
        new_ids = old_ids + list(add_ids)

        # Mapping row
        rows_two_cols.append({"Old_Combination": fmt_combo(old_ids),
                              "New_Combination": fmt_combo(new_ids)})

        # Build a fully filled row using schema of df_base
        row_dict = {col: 0 for col in df_base.columns}  # start with zero-ish
        # Keep identifiers
        row_dict["Old_Combination"] = fmt_combo(old_ids)
        row_dict["New_Combination"] = fmt_combo(new_ids)
        # Preserve "Combination" as the original old combo (if needed elsewhere)
        row_dict["Combination"] = fmt_combo(old_ids)

        # Fill the five C-blocks from the new_ids
        # (i.e., the final composition we evaluate is the new 5-client set)
        ok = True
        for blk, cid in zip(C_BLOCKS, new_ids):
            if cid not in df_clients.index:
                ok = False
                break
            fill_block(row_dict, blk, cid)
        if not ok:
            continue

        # Zero any remaining blocks (shouldn't happen if len(C_BLOCKS)==5 and we fill to 5)
        for blk in C_BLOCKS[len(new_ids):]:
            zero_block(row_dict, blk)

        # Compute globals from the blocks we just filled
        compute_globals(row_dict)
        rows_filled.append(row_dict)

# ---------- DataFrames & Save ----------
df_two_cols = pd.DataFrame(rows_two_cols).drop_duplicates().reset_index(drop=True)
df_filled   = pd.DataFrame(rows_filled)

two_cols_path = "Combinations_Old_vs_New_15to25_All_Len2to5.csv"
filled_path   = "MNIST_Federated_Combinations_Filled_All_Len2to5.csv"

df_two_cols.to_csv(two_cols_path, index=False)
df_filled.to_csv(filled_path, index=False)

print("‚úÖ Files saved:")
print(two_cols_path)
print(filled_path)
print("\nMapping preview:")
print(df_two_cols.head(8))
print("\nFilled preview:")
print(df_filled.head(5))

df_filled.to_csv("few_text_dataset_CIFAR10.csv")

"""**Unique Combinations (Phase I and Phase 2)**
---
"""

import pandas as pd

# Load the uploaded CSV file
file_path = "few_text_dataset_CIFAR10.csv"
df = pd.read_csv(file_path)

# Function to parse combination string like "C1-C2-C3"
def parse_combo(s):
    if pd.isna(s) or not isinstance(s, str) or not s:
        return []
    return [tok for tok in s.split('-') if tok.strip()]

# Filter only rows where Old_Combination length is 3 or 4
df["Old_Length"] = df["Old_Combination"].apply(lambda x: len(parse_combo(x)))
df_filtered = df[df["Old_Length"].isin([3, 4])].reset_index(drop=True)

# Save filtered dataset
filtered_path = "few_text_dataset_CIFAR10_filtered_3_4.csv"
df_filtered.to_csv(filtered_path, index=False)
df_filtered

df_filtered['Old_Length'].value_counts()

import re, random
import pandas as pd

# Load your file (adjust path if needed)
df = pd.read_csv("few_text_dataset_CIFAR10_filtered_3_4.csv")  # or "/mnt/data/few_text_dataset (1).csv"

def parse_set(s):
    # robust: grabs all integers regardless of spacing or separators
    return frozenset(map(int, re.findall(r"-?\d+", str(s))))

# Parse and make an order-invariant key
df["_set"] = df["New_Combination"].apply(parse_set)
df["_key"] = df["_set"].apply(lambda s: tuple(sorted(s)))

# Drop exact duplicates w.r.t. element set (ignores order)
df_unique = df.drop_duplicates(subset=["_key"]).copy()

# Randomly pick up to 100 unique rows
random_seed = 42  # set for reproducibility (or remove for true randomness)
sample_n = min(500, len(df_unique))
sampled = df_unique.sample(n=sample_n, random_state=random_seed)

# If you want them back as neat strings like "1,2,3"
sampled["normalized_combinations"] = sampled["_key"].apply(lambda t: ",".join(map(str, t)))

# Result: `sampled` holds your unique random 500
print(len(df), "rows ->", len(df_unique), "unique combos -> sampled", len(sampled))
sampled

sampled['Old_Length'].value_counts()

sampled.to_csv("few_text_dataset_1CIFAR10.csv")

"""**Unique Combinations (Manul Composition)**
---
"""

# =============================
# CONFIGURATION: Load from 'New_Combination'
# =============================
COMBO_CSV_PATH = "few_text_dataset_1CIFAR10.csv"
df_combos = pd.read_csv(COMBO_CSV_PATH)

# Extract and parse unique combinations like "C1-C3-C5"
combination_list = df_combos["New_Combination"].dropna().unique().tolist()
parsed_combinations = [
    [int(c.replace("C", "")) for c in combo.split("-") if c.startswith("C")]
    for combo in combination_list
]

EPOCHS = 5
BATCH = 64
FEATURE_COUNT = 32 * 32 * 3

print(f"‚úÖ Loaded {len(parsed_combinations)} unique combinations from {COMBO_CSV_PATH}")

# =============================
# Build Model Architecture (CIFAR-10)
# =============================
if TF_AVAILABLE:
    def build_model():
        model = models.Sequential([
            layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),
            layers.MaxPooling2D((2,2)),
            layers.Conv2D(64, (3,3), activation='relu'),
            layers.MaxPooling2D((2,2)),
            layers.Flatten(),
            layers.Dense(128, activation='relu'),
            layers.Dense(10, activation='softmax')
        ])
        model.compile(optimizer='adam',
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])
        return model

# =============================
# Prepare Global Test Set (CIFAR-10)
# =============================
if TF_AVAILABLE:
    all_images = np.concatenate([v[0] for v in clients_data.values()])
    all_labels = np.concatenate([v[1] for v in clients_data.values()])
    perm = np.random.permutation(len(all_images))

    x_test = (all_images[perm[:10000]] / 255.0).astype("float32")
    y_test = all_labels[perm[:10000]]

    print(f"‚úÖ Global CIFAR-10 test set prepared: {x_test.shape}, Labels: {y_test.shape}")

# =============================
# FedAvg + Aggregation Metrics
# =============================
def fedavg_weights(clients):
    total = sum(local_results[c]["samples"] for c in clients)
    w_sum = None
    for i, c in enumerate(clients):
        w = local_results[c]["weights"]
        factor = local_results[c]["samples"] / total
        if i == 0:
            w_sum = [wi * factor for wi in w]
        else:
            w_sum = [acc + wi * factor for acc, wi in zip(w_sum, w)]
    return w_sum

def evaluate_global_accuracy(clients):
    gmodel = build_model()
    gmodel.set_weights(fedavg_weights(clients))
    loss, acc = gmodel.evaluate(x_test, y_test, verbose=0)
    return float(acc * 100)

def aggregate_latency(clients):
    lats = [local_results[c]["latency_ms"] for c in clients]
    return float(sum(lats)), float(max(lats))

def aggregate_volume(clients):
    return int(sum(local_results[c]["samples"] for c in clients))

# =============================
# Simulate for New_Combination Entries (CIFAR-10)
# =============================
combo_records = []

if TF_AVAILABLE:
    for idx, client_ids in enumerate(parsed_combinations):

        # ====================================================
        # NEW: Start combination computation timer
        # ====================================================
        t_start_combo = time.time()

        local_results = {}
        label_distributions = {}

        print(f"\nüîÅ Simulating combination {idx + 1}/{len(parsed_combinations)}: {client_ids}")

        # ----- Local Training -----
        for cid in client_ids:

            x_c, y_c = clients_data[f"client_{cid}"]
            x_c = (x_c / 255.0).astype("float32")
            y_c = y_c.astype("int32")

            # Apply noise if defined
            if cid in PIXEL_NOISE_CLIENTS:
                x_c = apply_pixel_noise(x_c)
            if cid in LABEL_NOISE_CLIENTS:
                y_c = apply_label_noise(y_c)
            if cid in SHIFT_CLIENTS:
                x_c = apply_distribution_shift(x_c)

            # Train local client
            model = build_model()
            t0 = time.time()
            model.fit(x_c, y_c, epochs=EPOCHS, batch_size=BATCH, verbose=0)
            latency_ms = (time.time() - t0) * 1000.0
            loss, acc = model.evaluate(x_test, y_test, verbose=0)

            local_results[cid] = {
                "samples": int(x_c.shape[0]),
                "features": FEATURE_COUNT,
                "acc": float(acc * 100),
                "latency_ms": float(latency_ms),
                "weights": model.get_weights()
            }

        # ----- Global Aggregation -----
        gacc = evaluate_global_accuracy(client_ids)
        gsum, gmax = aggregate_latency(client_ids)
        gvol = aggregate_volume(client_ids)

        # ====================================================
        # NEW: End combination computation timer
        # ====================================================
        t_end_combo = time.time()
        computation_time_ms = (t_end_combo - t_start_combo) * 1000.0

        # ----- Save Record -----
        combo_str = '-'.join([f"C{c}" for c in client_ids])
        combo_records.append({
            "New_Combination": combo_str,
            "Global_Accuracy(%)": gacc,
            "Global_DataVolume": gvol,
            "Global_Latency_Sum(ms)": gsum,
            "Global_RoundTime_Max(ms)": gmax,

            # NEW FIELD ADDED
            "Computation_Time(ms)": computation_time_ms
        })

# =============================
# Save Final Results from New_Combination
# =============================
df_combo_results = pd.DataFrame(combo_records)
OUT_PATH = "Simulated_Results_From_New_Combinations_CIFAR10.csv"
df_combo_results.to_csv(OUT_PATH, index=False)

print(f"\n‚úÖ Saved simulated results for all New_Combination entries to: {OUT_PATH}")
